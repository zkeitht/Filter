{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numeric columns                         : 2956 columns\n",
      "Number of columns from all files processed at once: 300 columns\n",
      "Number of column chunks that can be generated     : 10 chunks\n",
      "Initialisation complete\n",
      "\n",
      "Selected numeric column chunks to be processed    : numcol chunk 1 to numcol chunk 10 (10 out of 10 numcol chunks)\n",
      "Number of files that will be generated            : 10 files\n",
      "\n",
      "\n",
      "\n",
      "read through all files\n",
      "\n",
      "numcol chunk 1 created\n",
      "137 rows\n",
      "298 columns left\n",
      "\n",
      "inf replaced with col max value\n",
      "cols with 2 unique values and below removed\n",
      "237 columns left\n",
      "\n",
      "column mean calculated\n",
      "\n",
      "all median within assumption, safe to reject mean/median == np.inf\n",
      "237 columns left\n",
      "cols with mean/median > 300 removed; if median = 0, cols with mean > 300 removed\n",
      "235 columns left\n",
      "\n",
      "235 columns left\n",
      "cols with |std/mean| < 0.1 removed\n",
      "231 columns left\n",
      "\n",
      "rejoined with the indices and smile columns\n",
      "233 columns left\n",
      "--- File exported as Filtered 0_14-1 (first) ----\n",
      "\n",
      "Numcol chunk 1 (first) completed. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "read through all files\n",
      "\n",
      "numcol chunk 2 created\n",
      "137 rows\n",
      "300 columns left\n",
      "\n",
      "inf replaced with col max value\n",
      "cols with 2 unique values and below removed\n",
      "300 columns left\n",
      "\n",
      "column mean calculated\n",
      "\n",
      "all median within assumption, safe to reject mean/median == np.inf\n",
      "300 columns left\n",
      "cols with mean/median > 300 removed; if median = 0, cols with mean > 300 removed\n",
      "220 columns left\n",
      "\n",
      "220 columns left\n",
      "cols with |std/mean| < 0.1 removed\n",
      "215 columns left\n",
      "\n",
      "--- File exported as Filtered 0_14-2 ----\n",
      "\n",
      "Numcol chunk 2 completed. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "read through all files\n",
      "\n",
      "numcol chunk 3 created\n",
      "137 rows\n",
      "300 columns left\n",
      "\n",
      "inf replaced with col max value\n",
      "cols with 2 unique values and below removed\n",
      "283 columns left\n",
      "\n",
      "column mean calculated\n",
      "\n",
      "all median within assumption, safe to reject mean/median == np.inf\n",
      "283 columns left\n",
      "cols with mean/median > 300 removed; if median = 0, cols with mean > 300 removed\n",
      "258 columns left\n",
      "\n",
      "258 columns left\n",
      "cols with |std/mean| < 0.1 removed\n",
      "236 columns left\n",
      "\n",
      "--- File exported as Filtered 0_14-3 ----\n",
      "\n",
      "Numcol chunk 3 completed. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "read through all files\n",
      "\n",
      "numcol chunk 4 created\n",
      "137 rows\n",
      "300 columns left\n",
      "\n",
      "inf replaced with col max value\n",
      "cols with 2 unique values and below removed\n",
      "115 columns left\n",
      "\n",
      "column mean calculated\n",
      "\n",
      "all median within assumption, safe to reject mean/median == np.inf\n",
      "115 columns left\n",
      "cols with mean/median > 300 removed; if median = 0, cols with mean > 300 removed\n",
      "113 columns left\n",
      "\n",
      "113 columns left\n",
      "cols with |std/mean| < 0.1 removed\n",
      "113 columns left\n",
      "\n",
      "--- File exported as Filtered 0_14-4 ----\n",
      "\n",
      "Numcol chunk 4 completed. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "read through all files\n",
      "\n",
      "numcol chunk 5 created\n",
      "137 rows\n",
      "300 columns left\n",
      "\n",
      "inf replaced with col max value\n",
      "cols with 2 unique values and below removed\n",
      "206 columns left\n",
      "\n",
      "column mean calculated\n",
      "\n",
      "all median within assumption, safe to reject mean/median == np.inf\n",
      "206 columns left\n",
      "cols with mean/median > 300 removed; if median = 0, cols with mean > 300 removed\n",
      "199 columns left\n",
      "\n",
      "199 columns left\n",
      "cols with |std/mean| < 0.1 removed\n",
      "195 columns left\n",
      "\n",
      "--- File exported as Filtered 0_14-5 ----\n",
      "\n",
      "Numcol chunk 5 completed. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "read through all files\n",
      "\n",
      "numcol chunk 6 created\n",
      "137 rows\n",
      "300 columns left\n",
      "\n",
      "inf replaced with col max value\n",
      "cols with 2 unique values and below removed\n",
      "252 columns left\n",
      "\n",
      "column mean calculated\n",
      "\n",
      "all median within assumption, safe to reject mean/median == np.inf\n",
      "252 columns left\n",
      "cols with mean/median > 300 removed; if median = 0, cols with mean > 300 removed\n",
      "227 columns left\n",
      "\n",
      "227 columns left\n",
      "cols with |std/mean| < 0.1 removed\n",
      "217 columns left\n",
      "\n",
      "--- File exported as Filtered 0_14-6 ----\n",
      "\n",
      "Numcol chunk 6 completed. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "read through all files\n",
      "\n",
      "numcol chunk 7 created\n",
      "137 rows\n",
      "300 columns left\n",
      "\n",
      "inf replaced with col max value\n",
      "cols with 2 unique values and below removed\n",
      "277 columns left\n",
      "\n",
      "column mean calculated\n",
      "\n",
      "all median within assumption, safe to reject mean/median == np.inf\n",
      "277 columns left\n",
      "cols with mean/median > 300 removed; if median = 0, cols with mean > 300 removed\n",
      "116 columns left\n",
      "\n",
      "116 columns left\n",
      "cols with |std/mean| < 0.1 removed\n",
      "111 columns left\n",
      "\n",
      "--- File exported as Filtered 0_14-7 ----\n",
      "\n",
      "Numcol chunk 7 completed. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "read through all files\n",
      "\n",
      "numcol chunk 8 created\n",
      "137 rows\n",
      "300 columns left\n",
      "\n",
      "inf replaced with col max value\n",
      "cols with 2 unique values and below removed\n",
      "0 columns left\n",
      "\n",
      "no column left, file will not be generated\n",
      "--- last file already exported as Filtered 0_14-7 --- \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "read through all files\n",
      "\n",
      "numcol chunk 9 created\n",
      "137 rows\n",
      "300 columns left\n",
      "\n",
      "inf replaced with col max value\n",
      "cols with 2 unique values and below removed\n",
      "0 columns left\n",
      "\n",
      "no column left, file will not be generated\n",
      "--- last file already exported as Filtered 0_14-7 --- \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "read through all files\n",
      "\n",
      "numcol chunk 10 created\n",
      "137 rows\n",
      "258 columns left\n",
      "\n",
      "inf replaced with col max value\n",
      "cols with 2 unique values and below removed\n",
      "0 columns left\n",
      "\n",
      "no column left, file will not be generated\n",
      "--- last file already exported as Filtered 0_14-7 --- \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- 6.1948 seconds ---(overall)\n",
      "--- Times exported as Step 2 Times 0_14 (10 chunks) ---\n"
     ]
    }
   ],
   "source": [
    "#### Step 2 ####\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "overall_start = time.time()\n",
    "\n",
    "## initialise timing\n",
    "steps = ['2.0   - Initialise import settings', \n",
    "         '2.1.1 - Read', \n",
    "         '2.1.2 - Concat', \n",
    "         '2.2   - Replace inf, uniq',\n",
    "         '2.3   - Mean',\n",
    "         '2.4   - Mean/median',\n",
    "         '2.5   - Std/mean',\n",
    "         '2.6   - Naming, exporting',\n",
    "         '2.1-6 - Colchunk time',\n",
    "         'Overall']\n",
    "times = {step:[] for step in steps}\n",
    "\n",
    "\n",
    "#### Step 2.0: initialise import settings ####\n",
    "start_time = time.time() # opt\n",
    "\n",
    "## names of files to filter:\n",
    "files = ['book_dataset_0','book_test_14']\n",
    "# files = ['book_dataset_0']\n",
    "# files = ['book_test_14']\n",
    "# files = ['book_dataset_17']\n",
    "# files = ['book_dataset_0','book_test_14', 'book_dataset_0_modifiedsubset_7']\n",
    "# files = ['book_dataset_0_modifiedsubset_7']\n",
    "\n",
    "\n",
    "## names of 'cleaner' version of files - (NA rows, [ ] removed): temp_filename\n",
    "# set files to None as they should not be used here\n",
    "tfiles, files = ['temp_'+ fname for fname in files], None\n",
    "\n",
    "## google drive path (set files to None here)\n",
    "# tfiles, files = ['/content/drive/MyDrive/Research Internship/temp_book_dataset_17',\n",
    "#                  '/content/drive/MyDrive/Research Internship/temp_book_dataset_18',\n",
    "#                  '/content/drive/MyDrive/Research Internship/temp_book_dataset_19',\n",
    "#                  '/content/drive/MyDrive/Research Internship/temp_book_dataset_27'], None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# read first 2 rows to obtain headers and total number of columns\n",
    "row1_2      = pd.read_csv('{}.csv'.format(tfiles[0]), nrows = 1, header = 0)\n",
    "descriptors = row1_2.columns\n",
    "\n",
    "# identify where the numeric columns starts (ignore indices & smiles now)\n",
    "start_numcol = 0\n",
    "for i in range(5):\n",
    "    if type(row1_2.iloc[0,i]) == str:\n",
    "        start_numcol = i+1\n",
    "        break\n",
    "\n",
    "# number of numeric columns\n",
    "n_numcol = len(descriptors) - start_numcol        \n",
    "\n",
    "# column chunk size to process\n",
    "cchunk_s = 300\n",
    "\n",
    "# number of column chunks (and hence files) that will be generated\n",
    "n_cchunk = len(descriptors)//cchunk_s + 1\n",
    "\n",
    "## col_ranges (to import certain column ranges of the files)\n",
    "# first col chunk without numcol - size: cchunk_s - start_numcol\n",
    "col_ranges = [range(start_numcol, cchunk_s)]\n",
    "# most col chunks - size: cchunk_s\n",
    "col_ranges.extend([range(start*cchunk_s, start*cchunk_s + cchunk_s) for start in range(1, n_numcol//cchunk_s)])\n",
    "# last col chunk - size: remainder of n_numcol/cchunk_s (< cchunk_s)\n",
    "# starts at\n",
    "start = (n_numcol//cchunk_s)*cchunk_s\n",
    "col_ranges.append(range(start, len(descriptors)))\n",
    "\n",
    "print('Number of numeric columns                         : {} columns'.format(n_numcol))\n",
    "print('Number of columns from all files processed at once: {} columns'.format(cchunk_s))\n",
    "print('Number of column chunks that can be generated     : {} chunks'.format(n_cchunk))\n",
    "\n",
    "if cchunk_s >= len(descriptors):\n",
    "    raise Exception('Column chunk size should be smaller than total number of columns ({})'.format(len(descriptors)))\n",
    "\n",
    "print('Initialisation complete\\n')\n",
    "times['2.0   - Initialise import settings'].append(time.time() - start_time)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## select only part of col_ranges\n",
    "# to select only one chunk, let part_start = n, part_end = n+1: e.g. 5th chunk: part_start = 4, part_start = 5\n",
    "# part_start = len(col_ranges)-1 # start at last chunk\n",
    "# part_start = len(col_ranges)-2 # start at 2nd last chunk\n",
    "part_start = 0\n",
    "part_end   = 1\n",
    "part_end   = len(col_ranges) # end at last chunk\n",
    "\n",
    "print('Selected numeric column chunks to be processed    : numcol chunk {} to numcol chunk {} ({} out of {} numcol chunks)'.format(part_start+1, part_end, part_end-part_start, n_cchunk))\n",
    "print('Number of files that will be generated            : {} files\\n\\n\\n'.format(part_end-part_start))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for count, col_range in enumerate(col_ranges[part_start:part_end], start = part_start + 1):\n",
    "    chunk_start = time.time()\n",
    "    \n",
    "    # empty list to store each imported file\n",
    "    df_files = []\n",
    "    \n",
    "    \n",
    "    #### Step 2.1: import selected column range for all files ####\n",
    "    start_time = time.time() # opt\n",
    "    \n",
    "    for tfile in tfiles:\n",
    "        \n",
    "        # number of rows to read at once\n",
    "        rchunk_s = 100\n",
    "        # setting row limit (nrows) prevents pandas to read the empty rows (after data) (happens to book-dataset1, doens't happen to book_dataset_0)\n",
    "        row_lim = pd.read_csv('{}.csv'.format(tfile), usecols = [0]).dropna().shape[0]\n",
    "        \n",
    "        # read a file in row chunks (from and up to col_range)\n",
    "        chunks  = pd.read_csv('{}.csv'.format(tfile), usecols = col_range, skiprows = 1, chunksize = rchunk_s, nrows = row_lim,  names = descriptors[col_range[0]:col_range[-1]+1], keep_default_na = False)\n",
    "        \n",
    "        # add the file to df_files\n",
    "        df_files.append(pd.concat(chunks))\n",
    "    \n",
    "    print('read through all files\\n')\n",
    "    times['2.1.1 - Read'].append(time.time() - start_time)\n",
    "\n",
    "    start_time = time.time() # opt\n",
    "\n",
    "    df_num = pd.concat(df_files)\n",
    "    # clear variables to reduce RAM usage\n",
    "    chunks   = None\n",
    "    df_files = None\n",
    "    print('numcol chunk {} created'.format(count))\n",
    "    print('{} rows'.format(df_num.shape[0]))\n",
    "    print('{} columns left\\n'.format(df_num.shape[1]))\n",
    "    times['2.1.2 - Concat'].append(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Step 2.2: replacing inf with column max value, remove columns with few unique values ####\n",
    "    start_time = time.time() # opt\n",
    "\n",
    "    df_num = df_num.apply(pd.to_numeric, errors = 'raise')\n",
    "    \n",
    "    # selcting only the columns that contain inf\n",
    "    # assuming that there are no NaN values right after importing (since all NaN rows are removed),\n",
    "    # so after replacing all infs with NaN, all NaNs are inf\n",
    "\n",
    "    # temporarily replaces infs with NaN\n",
    "    infsreplaced = df_num.mask(df_num == np.inf)\n",
    "    # columns without inf\n",
    "    noinf = infsreplaced.dropna(axis = 1)\n",
    "    # columns with inf\n",
    "    infs = df_num.drop(noinf.columns, axis = 1)\n",
    "\n",
    "    cols = infs.columns\n",
    "    ## replacing infs with col max value (by first excluding inf)\n",
    "    for col in cols:\n",
    "        # create a boolean Series (the mask) that has all inf as False\n",
    "        mask = infs[col] != np.inf\n",
    "        # setting inf to max (False values (inf) are converted to max in column (inf ignored by mask))\n",
    "        df_num.loc[~mask, col] = df_num.loc[mask, col].max()\n",
    "    print(\"inf replaced with col max value\")\n",
    "    \n",
    "    # clear variables to reduce RAM usage\n",
    "    infsreplaced = None\n",
    "    noinf        = None\n",
    "    infs         = None\n",
    "    \n",
    "    # number of unique values in a column\n",
    "    n_uniq_values = np.array(df_num.apply(lambda x: len(x.unique())))\n",
    "    ## remove columns that have (2) unique values and below\n",
    "    uniq_thres = 2\n",
    "    df_num = df_num.loc[:, n_uniq_values > uniq_thres]\n",
    "\n",
    "    print('cols with {} unique values and below removed'.format(uniq_thres))\n",
    "    print('{} columns left\\n'.format(df_num.shape[1]))\n",
    "    \n",
    "    # around col 1800-2100 , uniq values starts to be <2\n",
    "    # last file might not be generated in this case if whole numcol chunk falls after this range\n",
    "    if df_num.shape[1] == 0:\n",
    "        print('no column left, file will not be generated')\n",
    "        print('--- last file already exported as Filtered {}-{} --- \\n\\n\\n\\n\\n'.format(nums_, fi))\n",
    "        continue\n",
    "    \n",
    "    times['2.2   - Replace inf, uniq'].append(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Step 2.3: calculate mean, remove columns with 0 mean, dealing with inf mean ####\n",
    "    start_time = time.time() # opt\n",
    "    \n",
    "    ## mean\n",
    "\n",
    "    # splitting into n minor column chunks\n",
    "    n = 10\n",
    "    split = np.array_split(np.array(df_num), n, axis = 1)\n",
    "\n",
    "    # first calculation of mean will result in overflow error (ignored here)\n",
    "    # caused by a few columns with numbers that are too large,\n",
    "    # which result in inf mean (doesn't matter if median within assumption)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # nanmean ignores NaN if present\n",
    "        mean = [m for colchunk in split for m in np.nanmean(colchunk, axis = 0)]\n",
    "    \n",
    "    print('column mean calculated\\n')\n",
    "    times['2.3   - Mean'].append(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Step 2.4: remove columns with mean/median > mm_thres (300), or columns where median==0 but mean > mean_if_md0_thres (300) ####\n",
    "    start_time = time.time() # opt\n",
    "\n",
    "    # mm stands for mean/median\n",
    "    mm_thres          = 300\n",
    "    mean_if_md0_thres = 300\n",
    "\n",
    "    mean     = np.array(mean)\n",
    "    median   = np.array(df_num.median())\n",
    "\n",
    "    # assume that all median < max_calc_mean/mm_thres(300), returns warning otherwise\n",
    "    nrows            = df_num.shape[0] # should be around 4,050,000 (27 files * 150,000 rows)\n",
    "    max_stor_val     = np.finfo(1.0).max\n",
    "    max_calc_mean    = max_stor_val/nrows\n",
    "    assump_med       = max_calc_mean/mm_thres\n",
    "    out_med_colname  = df_num.columns[median > assump_med]\n",
    "    if  len(out_med_colname)>0:\n",
    "        print('****Warning: some median larger than assumed, outliers are {}****, check mean/median again for inf mean by scaling.'.format(out_med_colname))\n",
    "    else:\n",
    "        print('all median within assumption, safe to reject mean/median == np.inf')\n",
    "\n",
    "\n",
    "    # ignore errors caused by dividing by median = 0 (returns inf)\n",
    "    with np.errstate(divide='ignore', invalid = 'raise'):\n",
    "        mm = mean/median\n",
    "\n",
    "    ## mean/median < threshold - accept\n",
    "    ## mean =/= 0, median = 0  - accept only if mean < 300\n",
    "    print('{} columns left'.format(df_num.shape[1]))\n",
    "\n",
    "    df_num.loc['mean'] = mean\n",
    "    df_num = df_num.loc[:, np.array(mm<mm_thres) | (np.array(median==0) & np.array(mean<mean_if_md0_thres)) ]\n",
    "\n",
    "    mean = np.array(df_num.loc['mean'].values)\n",
    "    df_num.drop(index = 'mean', inplace = True)\n",
    "\n",
    "    print('cols with mean/median > {} removed; if median = 0, cols with mean > {} removed'.format(mm_thres, mean_if_md0_thres))\n",
    "    print('{} columns left\\n'.format(df_num.shape[1]))\n",
    "    times['2.4   - Mean/median'].append(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Step 2.5: removing columns that don't satisfy std/mean condition ####\n",
    "    start_time = time.time() # opt\n",
    "\n",
    "    # calculate std\n",
    "    # splitting into n column chunks\n",
    "    n = 10\n",
    "    split = np.array_split(np.array(df_num), n, axis = 1)\n",
    "\n",
    "    std      = np.array([s for colchunk in split for s in np.nanstd(colchunk, axis = 0)])\n",
    "\n",
    "    ## std/mean condition: std/mean > 0.1 - accept\n",
    "    stdmean_thres = 0.1\n",
    "\n",
    "    stdmean  = std/mean\n",
    "\n",
    "    print('{} columns left'.format(df_num.shape[1]))\n",
    "\n",
    "    df_num = df_num.loc[:, abs(stdmean) > stdmean_thres]\n",
    "\n",
    "    print('cols with |std/mean| < {} removed'.format(stdmean_thres))\n",
    "    print('{} columns left\\n'.format(df_num.shape[1]))\n",
    "    times['2.5   - Std/mean'].append(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Step 2.6: file naming, exporting ####\n",
    "    start_time = time.time() # opt\n",
    "\n",
    "    ## file naming\n",
    "    # take numbers only from each file\n",
    "    nums = []\n",
    "    for tfile in tfiles:\n",
    "        fnums = [char for char in tfile if char.isdigit()]\n",
    "        nums.append(''.join(fnums))\n",
    "\n",
    "    if len(nums) < 5:\n",
    "        nums_ = '_'.join(nums)\n",
    "    else: # 5 files or above\n",
    "        nums_ = '_'.join(nums[:2] +['...'] + nums[-2:])\n",
    "    \n",
    "    # file index (1 to n_cchunk)\n",
    "    if count == 1:\n",
    "        fi = str(count)+' (first)'\n",
    "        \n",
    "        ## concat only the FIRST numeric col chunk with smiles (and index)\n",
    "        # empty list to store each imported smile\n",
    "        df_smiles = []\n",
    "\n",
    "        for tfile in tfiles:\n",
    "\n",
    "            # number of rows to read at once\n",
    "            rchunk_s = 100\n",
    "            # setting row limit (nrows) prevents pandas to read the empty rows (after data) (happens to book-dataset1, doens't happen to book_dataset_0)\n",
    "            row_lim = pd.read_csv('{}.csv'.format(tfile), usecols = [0]).dropna().shape[0]\n",
    "\n",
    "            # read a file in row chunks (up to smiles column)\n",
    "            chunks  = pd.read_csv('{}.csv'.format(tfile), usecols = range(start_numcol), skiprows = 1, chunksize = rchunk_s, nrows = row_lim, names = descriptors[:start_numcol], keep_default_na = False)\n",
    "\n",
    "            # add the file to df_smiles\n",
    "            df_smiles.append(pd.concat(chunks))\n",
    "\n",
    "        df_smiles = pd.concat(df_smiles)\n",
    "        \n",
    "        # joining back the indices and smile columns\n",
    "        df_num = pd.concat([df_smiles, df_num], axis = 1)\n",
    "        print('rejoined with the indices and smile columns')\n",
    "        print('{} columns left'.format(df_num.shape[1]))\n",
    "        \n",
    "        # clear variables to reduce RAM usage\n",
    "        df_smiles = None\n",
    "        chunks    = None\n",
    "\n",
    "    elif count == n_cchunk:\n",
    "        fi = str(count)+' (last)'\n",
    "    else: # count == 2, 3, ... n_cchunk-1\n",
    "        fi = str(count)\n",
    "     \n",
    "    # export\n",
    "    df_num.to_csv('Filtered {}-{}.csv'.format(nums_, fi), index=False)\n",
    "    # display(df_num)\n",
    "    \n",
    "    # clear variable to reduce RAM usage\n",
    "    df_num = None\n",
    "\n",
    "    print('--- File exported as Filtered {}-{} ----\\n'.format(nums_, fi)) # opt\n",
    "    print('Numcol chunk {} completed. \\n\\n\\n\\n\\n'.format(fi))\n",
    "    times['2.6   - Naming, exporting'].append(time.time() - start_time)\n",
    "    times['2.1-6 - Colchunk time'].append(time.time() - chunk_start)\n",
    "    \n",
    "times['Overall'].append(time.time() - overall_start)\n",
    "print(\"--- {:.4f} seconds ---(overall)\".format(time.time() - overall_start))  #opt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## timing\n",
    "d = list(  zip( times.keys(), [np.mean(np.array(times[key])) for key in times], times.values() )  )\n",
    "time_df, time_fname = pd.DataFrame(data=d, columns = ['Step', 'Avg Time', 'Times']), 'Step 2 Times {} ({} chunks)'.format(nums_, n_cchunk)\n",
    "time_df.to_csv('{}.csv'.format(time_fname))\n",
    "print('--- Times exported as {} ---'.format(time_fname)) # opt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
