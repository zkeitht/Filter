{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first file/a file with header descriptors\n",
      "Not first file, using descriptors from book_dataset_0 as header\n",
      "read 202 rows\n",
      "NA rows removed\n",
      "137 rows left\n",
      "] removed in last column\n",
      "[ removed in nAcid\n",
      "2958 columns\n",
      "--- 9.8654 seconds ---\n",
      "numeric columns separated from the indices and smile columns\n",
      "2956 columns left\n",
      "inf replaced with col max value\n",
      "cols with 2 unique values and below removed\n",
      "1670 columns left\n",
      "--- 1.3671 seconds ---\n",
      "column mean calculated\n",
      "--- 0.0038 seconds ---\n",
      "all median within assumption, safe to reject mean/median == np.inf\n",
      "1670 columns left\n",
      "cols with mean/median > 300 removed; if median = 0, cols with mean > 300 removed\n",
      "1368 columns left\n",
      "--- 0.1808 seconds ---\n",
      "1368 columns left\n",
      "cols with |std/mean| < 0.1 removed\n",
      "1318 columns left\n",
      "rejoined with the indices and smile columns\n",
      "File exported as Filtered 0_14\n",
      "--- 0.5596 seconds ---\n",
      "--- 11.9788 seconds ---(overall)\n"
     ]
    }
   ],
   "source": [
    "#### Step 1 ####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "overall_start = time.time()\n",
    "\n",
    "## initialise timing\n",
    "steps = ['1.0   - Initialise import settings', \n",
    "         '1.1   - First row check', \n",
    "         '1.2   - Set row limit', \n",
    "         '1.3   - Concat Chunks (1 file)',\n",
    "         '1.4   - Remove []',\n",
    "         '1.5   - Export',\n",
    "         '1.1-5 - 1 file time (total)',\n",
    "         'Overall']\n",
    "\n",
    "times = {step:[] for step in steps}\n",
    "\n",
    "\n",
    "\n",
    "#### Step 1: import, remove space before inf, NA rows, ] in last column, [ in nAcid column, ####\n",
    "#### add header descriptors to each file, export as temp file                               ####\n",
    "\n",
    "#### 1.0   - Initialise import settings ####\n",
    "start_time = time.time() # opt\n",
    "\n",
    "## names of files to filter:\n",
    "files = ['book_dataset_0','book_test_14']\n",
    "# files = ['book_dataset_0']\n",
    "# files = ['book_test_14']\n",
    "# files = ['book_dataset_0_modifiedsubset_7']\n",
    "\n",
    "## remove .csv first when using google drive path, also uncomment google drive path - tfiles to get filename from path\n",
    "# files = ['/content/drive/MyDrive/completed sets/book_dataset_17',\n",
    "#          '/content/drive/MyDrive/completed sets/book_dataset_18',\n",
    "#          '/content/drive/MyDrive/completed sets/book_dataset_19',\n",
    "#          '/content/drive/MyDrive/completed sets/book_dataset_27']\n",
    "# files = files[2:]\n",
    "\n",
    "\n",
    "## names of 'cleaner' version of files - (NA rows, [ ] removed): temp_filename\n",
    "tfiles = ['temp_'+ fname for fname in files]\n",
    "\n",
    "## google drive path - tfiles\n",
    "# def find_name(paths):\n",
    "#     return [path[path.find('book'):] for path in paths]\n",
    "# tfiles = ['temp_'+ file for file in find_name(files)]\n",
    "\n",
    "\n",
    "first_file_w_descriptor = 'book_dataset_0'\n",
    "\n",
    "# to check if all the number of columns are the same\n",
    "ncol_check = []\n",
    "times['1.0   - Initialise import settings'].append(time.time() - start_time)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for count, file in enumerate(files):\n",
    "    file_time = time.time() # opt\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### 1.1   - First row check ####\n",
    "    start_time = time.time() # opt\n",
    "    \n",
    "    # checking whether first row is descriptor\n",
    "    first_row = pd.read_csv('{}.csv'.format(file), nrows = 0, header = 0)\n",
    "    \n",
    "    try:\n",
    "        float(first_row.columns[3])\n",
    "        print('Not first file, using descriptors from {} as header'.format(first_file_w_descriptor))\n",
    "        header = pd.read_csv('{}.csv'.format(first_file_w_descriptor), nrows = 0, header = 0)\n",
    "        descriptors = header.columns\n",
    "        skiprow = None\n",
    "\n",
    "    except ValueError:\n",
    "        print (\"This is the first file/a file with header descriptors\")\n",
    "        descriptors = first_row.columns\n",
    "        # skip first row (header) later since already imported headers\n",
    "        skiprow = 1\n",
    "    times['1.1   - First row check'].append(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### 1.2   - Chunk ####\n",
    "    start_time = time.time() # opt    \n",
    "    \n",
    "    # number of rows to read at once\n",
    "    rchunk_s = 100\n",
    "    # setting row limit (nrows) prevents pandas to read the empty rows (after data) (happens to book-dataset1, doens't happen to book_dataset_0)\n",
    "    row_lim = pd.read_csv('{}.csv'.format(file), usecols = [0], names = descriptors, skiprows = skiprow).dropna().shape[0]\n",
    "    # recognising \" 'NA'\" as NaN\n",
    "    navalues = \"'NA'\"\n",
    "    times['1.2   - Set row limit'].append(time.time() - start_time)\n",
    "\n",
    "    \n",
    "    # read a file in row chunks\n",
    "    chunks = pd.read_csv('{}.csv'.format(file), names = descriptors, skiprows = skiprow, chunksize = rchunk_s, nrows = row_lim, na_values = navalues, skipinitialspace = True)\n",
    "    \n",
    "    # Iteratively appending rows to a DataFrame can be more computationally intensive than a single concatenate.\n",
    "    # A better solution is to append those rows to a list \n",
    "    # and then concatenate the list with the original DataFrame all at once.\n",
    "    #    https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.append.html\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### 1.3   - Concat Chunks (1 file) ####\n",
    "    start_time = time.time() # opt\n",
    "    \n",
    "    # removing NA rows upon import, concat all row chunks (of a single file)\n",
    "    df  = pd.concat(chunks).dropna()\n",
    "    \n",
    "    print('Read {} rows in {}'.format(row_lim, file))\n",
    "    print('NA rows removed')\n",
    "    print('{} rows left'.format(df.shape[0]))\n",
    "    \n",
    "    times['1.3   - Concat Chunks (1 file)'].append(time.time() - start_time)\n",
    "    \n",
    "    # store number of columns of file to be checked later\n",
    "    ncol_check.append(df.shape[1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### 1.4   - Remove [] ####\n",
    "    start_time = time.time() # opt\n",
    "    \n",
    "    # removing ] in last column\n",
    "    if 'PubchemFP880' in df.columns:\n",
    "        if type(df['PubchemFP880'].iloc[-1]) == str and \"]\" in df['PubchemFP880'].iloc[-1]:\n",
    "            df['PubchemFP880'] = pd.to_numeric(df['PubchemFP880'].astype(str).str.rstrip(']'))\n",
    "            print('] removed in last column')\n",
    "        else:\n",
    "            print('] not present in last column')\n",
    "    else:\n",
    "        print('PubchemFP880 not among columns')\n",
    "\n",
    "    # removing [ in nAcid column\n",
    "    if 'nAcid' in df.columns:\n",
    "        if type(df['nAcid'].iloc[3]) == str and \"[\" in df['nAcid'].iloc[0]:\n",
    "            df['nAcid'] = pd.to_numeric(df['nAcid'].str.lstrip('['))\n",
    "            print('[ removed in nAcid')\n",
    "        else:\n",
    "            print('[ not present in nAcid')\n",
    "    else:\n",
    "        print('nAcid not among columns')\n",
    "    times['1.4   - Remove []'].append(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### 1.5   - Export ####\n",
    "    start_time = time.time() # opt\n",
    "    \n",
    "    df.to_csv('{}.csv'.format(tfiles[count]), index=False)\n",
    "    print('File exported as {}.csv'.format(tfiles[count]))\n",
    "    print()\n",
    "    times['1.5   - Export'].append(time.time() - start_time)\n",
    "    \n",
    "    times['1.1-5 - 1 file time (total)'].append(time.time() - file_time)\n",
    "    \n",
    "    # clear variable to reduce RAM usage\n",
    "    chunks = None\n",
    "    df     = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# make sure that all files have same number of cols\n",
    "if ncol_check.count(ncol_check[0]) == len(ncol_check):\n",
    "    print('All files have same number ({}) of columns'.format(ncol_check[0]))\n",
    "else:\n",
    "    print('****Warning: files have different numbers of columns****')\n",
    "    for tfile, ncol in zip(tfiles, ncol_check):\n",
    "        print ('{}: {} columns'.format(tfile, ncol))\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "## file naming\n",
    "# take numbers only from each file\n",
    "nums = []\n",
    "for tfile in tfiles:\n",
    "    fnums = [char for char in tfile if char.isdigit()]\n",
    "    nums.append(''.join(fnums))\n",
    "\n",
    "if len(nums) < 5:\n",
    "    nums_ = '_'.join(nums)\n",
    "else: # 5 files or above\n",
    "    nums_ = '_'.join(nums[:2] +['...'] + nums[-2:])\n",
    "    \n",
    "print(tfiles)\n",
    "times['Overall'].append(time.time() - overall_start)\n",
    "print(\"--- {:.4f} seconds ---(overall)\".format(time.time() - overall_start))  #opt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## timing\n",
    "d = list(  zip( times.keys(), [np.mean(np.array(times[key])) for key in times], times.values() )  )\n",
    "time_df, time_fname = pd.DataFrame(data=d, columns = ['Step', 'Avg Time', 'Times']), 'Step 1 Times {}'.format(nums_)\n",
    "time_df.to_csv('{}.csv'.format(time_fname))\n",
    "print('--- Times exported as {} ---'.format(time_fname)) # opt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
